{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qio32tYXxdk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image, ImageFilter\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import itertools\n",
        "\n",
        "# -------------------------\n",
        "# 1Ô∏è‚É£ Configuration\n",
        "# -------------------------\n",
        "zip_path = \"/content/drive/MyDrive/img_align_celeba.zip\"  # path to your uploaded ZIP\n",
        "extract_root = \"/content/celeba_images\"\n",
        "subset_folder = \"/content/celeba_images/subset_25k\"\n",
        "\n",
        "os.makedirs(extract_root, exist_ok=True)\n",
        "os.makedirs(subset_folder, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# 2Ô∏è‚É£ Extract ZIP\n",
        "# -------------------------\n",
        "print(\"üìÇ Extracting CelebA zip...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_root)\n",
        "\n",
        "# -------------------------\n",
        "# 3Ô∏è‚É£ Identify full image folder\n",
        "# -------------------------\n",
        "image_folder = os.path.join(extract_root, \"img_align_celeba\")\n",
        "if not os.path.exists(image_folder):\n",
        "    raise FileNotFoundError(\"Extracted image folder not found!\")\n",
        "\n",
        "all_images = sorted(os.listdir(image_folder))\n",
        "print(f\"Total images in full dataset: {len(all_images)}\")\n",
        "\n",
        "# -------------------------\n",
        "# 4Ô∏è‚É£ Copy first 25k images to subset\n",
        "# -------------------------\n",
        "subset_images = itertools.islice(all_images, 25000)\n",
        "\n",
        "for img in subset_images:\n",
        "    shutil.copy(os.path.join(image_folder, img), subset_folder)\n",
        "\n",
        "print(f\"Subset created with {len(os.listdir(subset_folder))} images\")\n",
        "print(f\"Subset folder path: {subset_folder}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHYD-f6UsC28",
        "outputId": "1b54dcfc-021a-434b-c793-b98d0e94bedb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Extracting CelebA zip...\n",
            "Total images in full dataset: 202599\n",
            "Subset created with 25000 images\n",
            "Subset folder path: /content/celeba_images/subset_25k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# Super-Resolution Full Pipeline (CelebA 2x, Local Images)\n",
        "# ===========================================\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) Use local CelebA folder\n",
        "# ----------------------------------------\n",
        "dataset_folder = \"/content/celeba_images/subset_25k\"  # your local folder with images\n",
        "all_images = [f for f in os.listdir(dataset_folder) if f.lower().endswith(\".jpg\")]\n",
        "print(f\"Found {len(all_images)} images in: {dataset_folder}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset class for LR-HR pairs\n",
        "# ----------------------------------------\n",
        "class CelebADataset(Dataset):\n",
        "    def __init__(self, root, scale=2, crop_size=64, max_images=None):\n",
        "        self.files = sorted([os.path.join(root,f) for f in os.listdir(root) if f.lower().endswith(\".jpg\")])\n",
        "        if max_images is not None:\n",
        "            self.files = self.files[:max_images]\n",
        "        self.scale = scale\n",
        "        self.crop_size = crop_size\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
        "        w,h = img.size\n",
        "        if w < self.crop_size or h < self.crop_size:\n",
        "            img = img.resize((max(w,self.crop_size), max(h,self.crop_size)), Image.BICUBIC)\n",
        "\n",
        "        # Random crop\n",
        "        x = torch.randint(0, img.width - self.crop_size + 1, (1,)).item()\n",
        "        y = torch.randint(0, img.height - self.crop_size + 1, (1,)).item()\n",
        "        hr = img.crop((x, y, x+self.crop_size, y+self.crop_size))\n",
        "        lr = hr.resize((self.crop_size//self.scale, self.crop_size//self.scale), Image.BICUBIC)\n",
        "\n",
        "        return self.to_tensor(lr), self.to_tensor(hr)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) DataLoader setup\n",
        "# ----------------------------------------\n",
        "scale = 2\n",
        "crop_size = 64\n",
        "batch_size = 16\n",
        "epochs = 50\n",
        "\n",
        "dataset = CelebADataset(dataset_folder, scale=scale, crop_size=crop_size)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "print(f\"DataLoader ready with {len(dataset)} samples\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Channel Attention module\n",
        "# ----------------------------------------\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels//reduction, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(channels//reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b,c,_,_ = x.size()\n",
        "        y = self.avg_pool(x).view(b,c)\n",
        "        y = self.fc(y).view(b,c,1,1)\n",
        "        return x * y\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) TinyESPCN Enhanced Model\n",
        "# ----------------------------------------\n",
        "class TinyESPCNEnhanced(nn.Module):\n",
        "    def __init__(self, scale=2, use_attention=True):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3,64,7,1,3)\n",
        "        self.res_blocks = nn.Sequential(*[nn.Sequential(nn.Conv2d(64,64,3,1,1), nn.ReLU()) for _ in range(10)])\n",
        "        if use_attention:\n",
        "            self.attention = ChannelAttention(64)\n",
        "        self.conv2 = nn.Conv2d(64, 3*(scale**2), 3,1,1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lr_input = x\n",
        "        x1 = F.relu(self.conv1(x))\n",
        "        x2 = self.res_blocks(x1)\n",
        "        if self.use_attention:\n",
        "            x2 = self.attention(x2)\n",
        "        x = self.pixel_shuffle(self.conv2(x2+x1))\n",
        "        lr_up = F.interpolate(lr_input, scale_factor=self.scale, mode='bicubic', align_corners=False)\n",
        "        return torch.clamp(x+lr_up,0,1)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Enhanced Loss (Perceptual + Edge + Lab)\n",
        "# ----------------------------------------\n",
        "class EnhancedLoss(nn.Module):\n",
        "    def __init__(self, device='cuda'):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.eval()\n",
        "        for p in vgg.parameters(): p.requires_grad=False\n",
        "        self.vgg = vgg.to(device)\n",
        "        self.device = device\n",
        "        self.layers = [2,7,12]\n",
        "\n",
        "        sobel_x = torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=torch.float32)\n",
        "        sobel_y = torch.tensor([[-1,-2,-1],[0,0,0],[1,2,1]], dtype=torch.float32)\n",
        "        laplacian = torch.tensor([[0,-1,0],[-1,4,-1],[0,-1,0]], dtype=torch.float32)\n",
        "\n",
        "        self.sobel_x = sobel_x.view(1,1,3,3).repeat(3,1,1,1).to(device)\n",
        "        self.sobel_y = sobel_y.view(1,1,3,3).repeat(3,1,1,1).to(device)\n",
        "        self.laplacian = laplacian.view(1,1,3,3).repeat(3,1,1,1).to(device)\n",
        "\n",
        "    def forward(self,sr,hr):\n",
        "        sr = torch.clamp(sr,0,1)\n",
        "        hr = torch.clamp(hr,0,1)\n",
        "\n",
        "        mean = torch.tensor([0.485,0.456,0.406],device=self.device).view(1,3,1,1)\n",
        "        std = torch.tensor([0.229,0.224,0.225],device=self.device).view(1,3,1,1)\n",
        "        sr_vgg = (sr-mean)/std\n",
        "        hr_vgg = (hr-mean)/std\n",
        "\n",
        "        loss=0\n",
        "        sr_f, hr_f = sr_vgg, hr_vgg\n",
        "        for i,layer in enumerate(self.vgg):\n",
        "            sr_f = layer(sr_f)\n",
        "            hr_f = layer(hr_f)\n",
        "            if i in self.layers:\n",
        "                loss += F.l1_loss(sr_f, hr_f)\n",
        "\n",
        "        grad_x_sr = F.conv2d(sr, self.sobel_x, padding=1, groups=3)\n",
        "        grad_y_sr = F.conv2d(sr, self.sobel_y, padding=1, groups=3)\n",
        "        grad_x_hr = F.conv2d(hr, self.sobel_x, padding=1, groups=3)\n",
        "        grad_y_hr = F.conv2d(hr, self.sobel_y, padding=1, groups=3)\n",
        "        edge_loss = F.l1_loss(grad_x_sr,grad_x_hr)+F.l1_loss(grad_y_sr,grad_y_hr)\n",
        "\n",
        "        lap_sr = F.conv2d(sr,self.laplacian,padding=1,groups=3)\n",
        "        lap_hr = F.conv2d(hr,self.laplacian,padding=1,groups=3)\n",
        "        edge_loss += F.l1_loss(lap_sr, lap_hr)\n",
        "\n",
        "        loss += 0.2 * edge_loss\n",
        "\n",
        "        sr_lab = rgb_to_lab(sr)\n",
        "        hr_lab = rgb_to_lab(hr)\n",
        "        loss += 0.1 * F.l1_loss(sr_lab, hr_lab)\n",
        "\n",
        "        return loss\n",
        "\n",
        "def rgb_to_lab(tensor):\n",
        "    from skimage import color\n",
        "    B,C,H,W = tensor.shape\n",
        "    lab=[]\n",
        "    for i in range(B):\n",
        "        img = tensor[i].detach().permute(1,2,0).cpu().numpy()\n",
        "        lab_img = color.rgb2lab(img)\n",
        "        lab.append(torch.tensor(lab_img, device=tensor.device).permute(2,0,1))\n",
        "    return torch.stack(lab)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) Training function\n",
        "# ----------------------------------------\n",
        "def train_model(model, dataloader, epochs=50, lr=1e-3, device='cuda'):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = EnhancedLoss(device=device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        pbar = tqdm(dataloader)\n",
        "        for lr_img, hr_img in pbar:\n",
        "            lr_img, hr_img = lr_img.to(device), hr_img.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            sr = model(lr_img)\n",
        "            loss = criterion(sr, hr_img)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pbar.set_description(f\"Epoch {epoch+1}/{epochs} Loss:{loss.item():.6f}\")\n",
        "    return model\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Initialize and train model\n",
        "# ----------------------------------------\n",
        "model = TinyESPCNEnhanced(scale=scale)\n",
        "print(\"Model initialized, starting training...\")\n",
        "model = train_model(model, loader, epochs=50, lr=1e-3, device=device)\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UWOQ0DEs8JT",
        "outputId": "98240103-07ab-4f27-c271-77e7f198ceee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Found 25000 images in: /content/celeba_images/subset_25k\n",
            "DataLoader ready with 25000 samples\n",
            "Model initialized, starting training...\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 548M/548M [00:07<00:00, 72.9MB/s]\n",
            "Epoch 1/50 Loss:2.950243: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:44<00:00, 14.99it/s]\n",
            "Epoch 2/50 Loss:2.959672: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:44<00:00, 14.93it/s]\n",
            "Epoch 3/50 Loss:3.012973: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:42<00:00, 15.21it/s]\n",
            "Epoch 4/50 Loss:2.712124: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:41<00:00, 15.41it/s]\n",
            "Epoch 5/50 Loss:2.699619: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:42<00:00, 15.28it/s]\n",
            "Epoch 6/50 Loss:2.529857: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:42<00:00, 15.32it/s]\n",
            "Epoch 7/50 Loss:2.502214: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.49it/s]\n",
            "Epoch 8/50 Loss:2.545166: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.51it/s]\n",
            "Epoch 9/50 Loss:2.199218: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.59it/s]\n",
            "Epoch 10/50 Loss:3.131749: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.51it/s]\n",
            "Epoch 11/50 Loss:2.349300: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.50it/s]\n",
            "Epoch 12/50 Loss:2.250749: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.57it/s]\n",
            "Epoch 13/50 Loss:2.357776: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:41<00:00, 15.41it/s]\n",
            "Epoch 14/50 Loss:3.161283: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.57it/s]\n",
            "Epoch 15/50 Loss:3.090080: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.48it/s]\n",
            "Epoch 16/50 Loss:2.595618: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.54it/s]\n",
            "Epoch 17/50 Loss:3.012240: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.58it/s]\n",
            "Epoch 18/50 Loss:2.880912: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.59it/s]\n",
            "Epoch 19/50 Loss:3.328425: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.54it/s]\n",
            "Epoch 20/50 Loss:2.485761: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.55it/s]\n",
            "Epoch 21/50 Loss:2.048162: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:42<00:00, 15.20it/s]\n",
            "Epoch 22/50 Loss:2.854222: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.54it/s]\n",
            "Epoch 23/50 Loss:3.045748: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.67it/s]\n",
            "Epoch 24/50 Loss:2.816776: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.71it/s]\n",
            "Epoch 25/50 Loss:3.248670: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.58it/s]\n",
            "Epoch 26/50 Loss:2.901746: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:41<00:00, 15.42it/s]\n",
            "Epoch 27/50 Loss:2.564640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:42<00:00, 15.27it/s]\n",
            "Epoch 28/50 Loss:2.529164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.58it/s]\n",
            "Epoch 29/50 Loss:2.262556: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.57it/s]\n",
            "Epoch 30/50 Loss:2.904386: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.67it/s]\n",
            "Epoch 31/50 Loss:2.567855: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.49it/s]\n",
            "Epoch 32/50 Loss:2.817743: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.64it/s]\n",
            "Epoch 33/50 Loss:2.160103: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.73it/s]\n",
            "Epoch 34/50 Loss:2.583898: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.60it/s]\n",
            "Epoch 35/50 Loss:1.981640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.63it/s]\n",
            "Epoch 36/50 Loss:2.514459: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:38<00:00, 15.89it/s]\n",
            "Epoch 37/50 Loss:2.929876: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:38<00:00, 15.87it/s]\n",
            "Epoch 38/50 Loss:2.557050: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:38<00:00, 15.94it/s]\n",
            "Epoch 39/50 Loss:2.759094: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:43<00:00, 15.04it/s]\n",
            "Epoch 40/50 Loss:3.516340: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:41<00:00, 15.43it/s]\n",
            "Epoch 41/50 Loss:2.817796: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.57it/s]\n",
            "Epoch 42/50 Loss:2.538816: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.63it/s]\n",
            "Epoch 43/50 Loss:2.557647: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.68it/s]\n",
            "Epoch 44/50 Loss:2.864372: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:40<00:00, 15.60it/s]\n",
            "Epoch 45/50 Loss:2.740535: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.64it/s]\n",
            "Epoch 46/50 Loss:2.411662: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.68it/s]\n",
            "Epoch 47/50 Loss:2.269907: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.74it/s]\n",
            "Epoch 48/50 Loss:2.500990: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.74it/s]\n",
            "Epoch 49/50 Loss:2.654735: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.76it/s]\n",
            "Epoch 50/50 Loss:2.839948: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [01:39<00:00, 15.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"tiny_espcn_celeba.pth\")\n",
        "print(\"Model weights saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqUX9qTUuEpV",
        "outputId": "323557f7-794c-4642-b4f7-615ba922843e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"tiny_espcn_celeba_full.pth\")\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6miFsG_B6g7",
        "outputId": "00481bb1-624a-44d4-9300-dea1aa9b9910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PqtgZWGILFeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}