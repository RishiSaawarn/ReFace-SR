{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qio32tYXxdk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image, ImageFilter\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LKeghdXKYKt5"
      },
      "outputs": [],
      "source": [
        "# ===========================================\n",
        "# Download LSDIR from Hugging Face into Colab\n",
        "# ===========================================\n",
        "!pip install datasets --quiet\n",
        "\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "# 1) Define where to store the dataset locally\n",
        "dataset_dir = \"/content/LSDIR\"\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# 2) Load LSDIR from Hugging Face\n",
        "# Using \"danjacobellis/LSDIR\" which contains LR-HR pairs and ~85k images\n",
        "dataset = load_dataset(\"danjacobellis/LSDIR\")\n",
        "\n",
        "# 3) Save LR-HR images locally for training\n",
        "def save_images(split=\"train\"):\n",
        "    lr_folder = os.path.join(dataset_dir, split, \"LR\", \"x2\")\n",
        "    hr_folder = os.path.join(dataset_dir, split, \"HR\")\n",
        "    os.makedirs(lr_folder, exist_ok=True)\n",
        "    os.makedirs(hr_folder, exist_ok=True)\n",
        "\n",
        "    for i, item in enumerate(dataset[split]):\n",
        "        # item has keys like 'lr' and 'hr' as bytes\n",
        "        lr_img = Image.open(item['lr'])\n",
        "        hr_img = Image.open(item['hr'])\n",
        "        lr_img.save(os.path.join(lr_folder, f\"{i:05d}.png\"))\n",
        "        hr_img.save(os.path.join(hr_folder, f\"{i:05d}.png\"))\n",
        "\n",
        "    return lr_folder, hr_folder\n",
        "\n",
        "lr_train_folder, hr_train_folder = save_images(\"train\")\n",
        "lr_test_folder, hr_test_folder = save_images(\"test\")\n",
        "\n",
        "# 4) Verify\n",
        "print(\"LR Train Folder:\", lr_train_folder)\n",
        "print(\"HR Train Folder:\", hr_train_folder)\n",
        "print(\"LR Test Folder:\", lr_test_folder)\n",
        "print(\"HR Test Folder:\", hr_test_folder)\n",
        "\n",
        "print(\"Sample LR images:\", os.listdir(lr_train_folder)[:5])\n",
        "print(\"Sample HR images:\", os.listdir(hr_train_folder)[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECeBwO40XoEZ"
      },
      "outputs": [],
      "source": [
        "# ===========================================\n",
        "# Super-Resolution Full Pipeline for LSDIR Ã—2 (No Augmentation)\n",
        "# ===========================================\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) Dataset class for pre-built LR-HR pairs (no augmentation)\n",
        "# ----------------------------------------\n",
        "class LSDIRDataset(Dataset):\n",
        "    def __init__(self, lr_folder, hr_folder):\n",
        "        self.lr_files = sorted(glob.glob(os.path.join(lr_folder, \"*.*\")))\n",
        "        self.hr_files = sorted(glob.glob(os.path.join(hr_folder, \"*.*\")))\n",
        "        if len(self.lr_files) == 0 or len(self.hr_files) == 0:\n",
        "            raise ValueError(\"No valid images found in LR or HR folder.\")\n",
        "        assert len(self.lr_files) == len(self.hr_files), \"LR and HR folder must have same number of images\"\n",
        "\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lr_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lr = Image.open(self.lr_files[idx]).convert(\"RGB\")\n",
        "        hr = Image.open(self.hr_files[idx]).convert(\"RGB\")\n",
        "        lr = self.to_tensor(lr)\n",
        "        hr = self.to_tensor(hr)\n",
        "        return lr, hr\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Channel Attention module\n",
        "# ----------------------------------------\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels//reduction, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(channels//reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        b,c,_,_ = x.size()\n",
        "        y = self.avg_pool(x).view(b,c)\n",
        "        y = self.fc(y).view(b,c,1,1)\n",
        "        return x * y\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) TinyESPCN Enhanced Model\n",
        "# ----------------------------------------\n",
        "class TinyESPCNEnhanced(nn.Module):\n",
        "    def __init__(self, scale=2, use_attention=True):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3,64,7,1,3)\n",
        "        self.res_blocks = nn.Sequential(*[nn.Sequential(nn.Conv2d(64,64,3,1,1), nn.ReLU()) for _ in range(10)])\n",
        "        if use_attention:\n",
        "            self.attention = ChannelAttention(64)\n",
        "        self.conv2 = nn.Conv2d(64, 3*(scale**2), 3,1,1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lr_input = x\n",
        "        x1 = F.relu(self.conv1(x))\n",
        "        x2 = self.res_blocks(x1)\n",
        "        if self.use_attention:\n",
        "            x2 = self.attention(x2)\n",
        "        x = self.pixel_shuffle(self.conv2(x2+x1))\n",
        "        lr_up = F.interpolate(lr_input, scale_factor=self.scale, mode='bicubic', align_corners=False)\n",
        "        return torch.clamp(x+lr_up,0,1)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Enhanced Loss (Perceptual + Edge + Lab)\n",
        "# ----------------------------------------\n",
        "class EnhancedLoss(nn.Module):\n",
        "    def __init__(self, device='cuda'):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.eval()\n",
        "        for p in vgg.parameters(): p.requires_grad=False\n",
        "        self.vgg = vgg.to(device)\n",
        "        self.device = device\n",
        "        self.layers = [2,7,12]\n",
        "\n",
        "        sobel_x = torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=torch.float32)\n",
        "        sobel_y = torch.tensor([[-1,-2,-1],[0,0,0],[1,2,1]], dtype=torch.float32)\n",
        "        laplacian = torch.tensor([[0,-1,0],[-1,4,-1],[0,-1,0]], dtype=torch.float32)\n",
        "\n",
        "        self.sobel_x = sobel_x.view(1,1,3,3).repeat(3,1,1,1).to(device)\n",
        "        self.sobel_y = sobel_y.view(1,1,3,3).repeat(3,1,1,1).to(device)\n",
        "        self.laplacian = laplacian.view(1,1,3,3).repeat(3,1,1,1).to(device)\n",
        "\n",
        "    def forward(self,sr,hr):\n",
        "        sr = torch.clamp(sr,0,1)\n",
        "        hr = torch.clamp(hr,0,1)\n",
        "\n",
        "        mean = torch.tensor([0.485,0.456,0.406],device=self.device).view(1,3,1,1)\n",
        "        std = torch.tensor([0.229,0.224,0.225],device=self.device).view(1,3,1,1)\n",
        "        sr_vgg = (sr-mean)/std\n",
        "        hr_vgg = (hr-mean)/std\n",
        "\n",
        "        loss=0\n",
        "        sr_f, hr_f = sr_vgg, hr_vgg\n",
        "        for i,layer in enumerate(self.vgg):\n",
        "            sr_f = layer(sr_f)\n",
        "            hr_f = layer(hr_f)\n",
        "            if i in self.layers:\n",
        "                loss += F.l1_loss(sr_f, hr_f)\n",
        "\n",
        "        grad_x_sr = F.conv2d(sr, self.sobel_x, padding=1, groups=3)\n",
        "        grad_y_sr = F.conv2d(sr, self.sobel_y, padding=1, groups=3)\n",
        "        grad_x_hr = F.conv2d(hr, self.sobel_x, padding=1, groups=3)\n",
        "        grad_y_hr = F.conv2d(hr, self.sobel_y, padding=1, groups=3)\n",
        "        edge_loss = F.l1_loss(grad_x_sr,grad_x_hr)+F.l1_loss(grad_y_sr,grad_y_hr)\n",
        "\n",
        "        lap_sr = F.conv2d(sr,self.laplacian,padding=1,groups=3)\n",
        "        lap_hr = F.conv2d(hr,self.laplacian,padding=1,groups=3)\n",
        "        edge_loss += F.l1_loss(lap_sr, lap_hr)\n",
        "\n",
        "        loss += 0.2 * edge_loss\n",
        "\n",
        "        sr_lab = rgb_to_lab(sr)\n",
        "        hr_lab = rgb_to_lab(hr)\n",
        "        loss += 0.1 * F.l1_loss(sr_lab, hr_lab)\n",
        "\n",
        "        return loss\n",
        "\n",
        "def rgb_to_lab(tensor):\n",
        "    from skimage import color\n",
        "    B,C,H,W = tensor.shape\n",
        "    lab=[]\n",
        "    for i in range(B):\n",
        "        img = tensor[i].detach().permute(1,2,0).cpu().numpy()\n",
        "        lab_img = color.rgb2lab(img)\n",
        "        lab.append(torch.tensor(lab_img, device=tensor.device).permute(2,0,1))\n",
        "    return torch.stack(lab)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) Training function\n",
        "# ----------------------------------------\n",
        "def train_model(model, dataloader, epochs=50, lr=1e-3, device='cuda'):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = EnhancedLoss(device=device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        pbar = tqdm(dataloader)\n",
        "        for lr_img, hr_img in pbar:\n",
        "            lr_img, hr_img = lr_img.to(device), hr_img.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            sr = model(lr_img)\n",
        "            loss = criterion(sr, hr_img)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pbar.set_description(f\"Epoch {epoch+1}/{epochs} Loss:{loss.item():.6f}\")\n",
        "    return model\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Upscale, sharpen, and MSAA post-processing\n",
        "# ----------------------------------------\n",
        "def upscale_images(model, files, out_folder=\"upscaled\", device='cuda', sharpen=True):\n",
        "    os.makedirs(out_folder, exist_ok=True)\n",
        "    model.eval()\n",
        "    to_tensor = transforms.ToTensor()\n",
        "    to_pil = transforms.ToPILImage()\n",
        "    with torch.no_grad():\n",
        "        for path in tqdm(files):\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            lr = to_tensor(img).unsqueeze(0).to(device)\n",
        "            sr = model(lr).clamp(0,1).cpu().squeeze(0)\n",
        "            out_img = to_pil(sr)\n",
        "            if sharpen:\n",
        "                out_img = out_img.filter(ImageFilter.UnsharpMask(radius=1.2, percent=100, threshold=1))\n",
        "            out_img.save(os.path.join(out_folder, os.path.basename(path)))\n",
        "\n",
        "def msaa_postprocess(input_folder=\"upscaled\", output_folder=\"upscaled_aa\", supersample=2):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    files = glob.glob(os.path.join(input_folder,\"*.*\"))\n",
        "    files = [f for f in files if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))]\n",
        "    for path in tqdm(files):\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        w,h = img.size\n",
        "        img = img.resize((w*supersample,h*supersample), Image.LANCZOS)\n",
        "        img = img.filter(ImageFilter.GaussianBlur(0.5))\n",
        "        img = img.resize((w,h), Image.LANCZOS)\n",
        "        img.save(os.path.join(output_folder, os.path.basename(path)))\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) Initialize dataset, DataLoader, and model\n",
        "# ----------------------------------------\n",
        "scale = 2\n",
        "batch_size = 16\n",
        "epochs = 50\n",
        "\n",
        "lr_folder = \"lr_train_folder\"\n",
        "hr_folder = \"hr_train_folder\"\n",
        "\n",
        "dataset = LSDIRDataset(lr_folder, hr_folder)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "print(f\"Dataset created with {len(dataset)} samples\")\n",
        "\n",
        "model = TinyESPCNEnhanced(scale=scale)\n",
        "print(\"Model initialized and ready for training\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Start training\n",
        "# ----------------------------------------\n",
        "trained_model = train_model(model, loader, epochs=epochs, lr=1e-4, device=device)\n",
        "print(\"Training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"tiny_espcn_lsdir.pth\")\n",
        "print(\"Model weights saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqUX9qTUuEpV",
        "outputId": "323557f7-794c-4642-b4f7-615ba922843e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"tiny_espcn_celeba_full.pth\")\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6miFsG_B6g7",
        "outputId": "00481bb1-624a-44d4-9300-dea1aa9b9910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PqtgZWGILFeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}